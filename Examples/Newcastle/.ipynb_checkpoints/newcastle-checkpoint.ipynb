{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newcastle (w/ Value Constraints)\n",
    "\n",
    "\n",
    "### Load and downsample constraints\n",
    "\n",
    "First, we load the interpretations from our digital outcrop model (DOM). These are spaced every ~1 cm, which is unnecessarily high resolution for our 3D model, so we also downsample them using `scipy.kdtree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import curlew\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "curlew.dtype = torch.float64 # smaller coordinates require higher precision\n",
    "curlew.device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting params\n",
    "from matplotlib import colors\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "font_dirs = ['../Helvet/',]\n",
    "font_files = font_manager.findSystemFonts(fontpaths=font_dirs[0], fontext=\"ttf\")\n",
    "\n",
    "for font_file in font_files:\n",
    "    font_manager.fontManager.addfont(font_file)\n",
    "\n",
    "# Plotting params\n",
    "# RCParams\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"text.usetex\"] = False\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Helvetica'\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"stixsans\"\n",
    "plt.rcParams[\"mathtext.default\"] = \"it\"\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['axes.formatter.use_mathtext'] = True\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams[\"axes.formatter.limits\"] = [-2, 2]\n",
    "\n",
    "# Curlew colormap\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Define the colors extracted manually from the provided logo image\n",
    "colors = [\n",
    "    \"#A6340B\",  # rich red (not darkest)\n",
    "    \"#E35B0E\",  # vibrant orange-red\n",
    "    \"#F39C12\",  # medium orange\n",
    "    \"#F0C419\",  # bright orange-yellow\n",
    "    \"#FAE8B6\",  # soft pale orange (close to white but not pure white)\n",
    "    \"#8CD9E0\",  # light cyan blue\n",
    "    \"#31B4C2\",  # medium cyan-blue\n",
    "    \"#1B768F\",  # medium blue \n",
    "    \"#054862\",  # deeper blue (not darkest)\n",
    "]\n",
    "\n",
    "# Create a discrete colormap using these colors\n",
    "curlew_cmap = mcolors.ListedColormap(colors)\n",
    "\n",
    "# Random seed\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load planar orientation (\"compass\") measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "planes = pd.read_csv('traces/newcastle_planes.csv')\n",
    "planes['Unit'] = [ t.split('.interpretation.')[1].split('.')[0] for t in planes['Name'] ] # shorten names so they are less ugly\n",
    "\n",
    "for k in ['Sample_Radius', 'RMS', 'Gx', 'Gy', 'Gz', 'Length', 'Name']: # delete unwanted columns\n",
    "    del planes[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and downsample structure normal (SNE) estimates\n",
    "\n",
    "Combine with the above \"compass\" measurements to our value constraint arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curlew.io import loadPLY\n",
    "fault = loadPLY('traces/FaultSNE.ply')\n",
    "dykeL = loadPLY('traces/DykeBottom.ply')\n",
    "dykeU = loadPLY('traces/DykeTop.ply')\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "scale = 0.5 # average points within this many meters\n",
    "for s in [fault, dykeL, dykeU]:\n",
    "    tree = KDTree( s['xyz'] ) # build KD-tree object\n",
    "    collapsed = np.full( (len(s['xyz']),), False ) # init array used to track which points have already been averaged\n",
    "    # loop through points and do averaging / collapsing\n",
    "    points = []\n",
    "    normals = []\n",
    "    for j in range(len(s['xyz'])):\n",
    "        if collapsed[j]:\n",
    "            continue # ignore point - it has already been collapsed!\n",
    "        else:\n",
    "\n",
    "            # get neighbourhood points\n",
    "            idx = tree.query_ball_point(s['xyz'][ j, : ], r = scale )\n",
    "\n",
    "            # average point location - this is our subsampled position\n",
    "            points.append( np.mean( s['xyz'][idx, : ], axis=0 ) ) # average position vector\n",
    "            normals.append( np.mean( s['normals'][idx, : ], axis=0 ) ) # average normal vector\n",
    "            normals[-1] = normals[-1] / np.linalg.norm(normals[-1]) # normalise to length 1\n",
    "\n",
    "            # flag points that are already moved to the output (condensed)\n",
    "            collapsed[idx] = True\n",
    "    s['xyz'] = np.array(points)\n",
    "    s['normals'] = np.array(normals)\n",
    "    if 'attr' in s: del s['attr'] # not needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_constraints = {\n",
    "    'Dyke' : ( np.vstack( [dykeU['xyz'], dykeL['xyz']] ), np.vstack( [dykeU['normals'], dykeL['normals']] ) ),\n",
    "    'Fault' : (fault['xyz'], fault['normals']),\n",
    "}\n",
    "\n",
    "for u in np.unique( planes['Unit'] ):\n",
    "    xyz = np.array( planes[ planes['Unit'] == u][['Cx','Cy','Cz']] ) # position\n",
    "    klm = np.array( planes[ planes['Unit'] == u][['Nx','Ny','Nz']] ) # normal\n",
    "    if u in gradient_constraints:\n",
    "        gradient_constraints[u] = (np.vstack([ gradient_constraints[u][0], xyz]), np.vstack([ gradient_constraints[u][1], klm]) )\n",
    "    else:\n",
    "        gradient_constraints[u] = (xyz, klm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure dyke gradient constraints point in the right direction\n",
    "gradient_constraints['Dyke'][1][ gradient_constraints['Dyke'][1][:,0] > 0 ] *= -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load contact traces, downsample them and convert to value constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 826 points for Dyke.Upper ... 20 points remain!\n",
      "Filtering 903 points for Dyke.Lower ... 21 points remain!\n",
      "Filtering 514 points for TopBarBeachShales ... 13 points remain!\n",
      "Filtering 463 points for TopDudleyCoal ... 11 points remain!\n",
      "Filtering 1080 points for TopBogeyHole ... 26 points remain!\n",
      "Filtering 906 points for Fault ... 20 points remain!\n"
     ]
    }
   ],
   "source": [
    "traces = pd.read_csv('traces/newcastle_traces.csv')\n",
    "traces['Unit'] = [ t.split('.interpretation.')[1].replace(' Boundary','').replace('.Trace','') for t in traces['Name'] ] # shorten names so they are less ugly\n",
    "\n",
    "for k in ['Name','Point_id', 'Cost', 'Cost_Mode']: # delete unwanted columns\n",
    "  del traces[k]\n",
    "\n",
    "unit_codes = {'Dyke.Upper' : 0.25, 'Dyke.Lower':-0.25, # N.B. dyke is ~0.5 m thick\n",
    "        'TopBarBeachShales' : 22.2, # in this case we use the approx meters above sea level as the scalar value, as these are relatively flat beds\n",
    "        'TopDudleyCoal' : 21.2,\n",
    "        'TopBogeyHole' : 18.,\n",
    "        'Fault' : 0., \n",
    "        # N.B. all other Unit names in the traces file will be ignored\n",
    "        }\n",
    "\n",
    "\n",
    "# loop through each lithology type and subsample contact points\n",
    "from scipy.spatial import KDTree\n",
    "value_constraints = {}\n",
    "for n,sv in unit_codes.items():\n",
    "  # find points of this lithology\n",
    "  mask = np.array(traces['Unit']) == n # which points are in this lithology?\n",
    "  c = np.sum(mask) # count points in this lithology\n",
    "  print(\"Filtering %d points for %s ... \" %(c, n ), end='' )\n",
    "\n",
    "  # construct a KD tree\n",
    "  xyz = np.array([traces[k] for k in ['Start_x', 'Start_y', 'Start_z']]).T\n",
    "  _xyz = xyz[mask, : ] # subset main point array\n",
    "  tree = KDTree( _xyz ) # build KD-tree object\n",
    "\n",
    "  # init array used to track which points have already been collapsed\n",
    "  collapsed = np.full( (c,), False )\n",
    "\n",
    "  # loop through points and do averaging / collapsing\n",
    "  points = []\n",
    "  for j in range(c):\n",
    "      if collapsed[j]:\n",
    "        continue # ignore point - it has already been collapsed!\n",
    "      else:\n",
    "\n",
    "        # get neighbourhood points\n",
    "        idx = tree.query_ball_point(_xyz[ j, : ], r = scale )\n",
    "\n",
    "        # average point location - this is our subsampled position\n",
    "        points.append( np.mean( _xyz[idx, : ], axis=0 ) )\n",
    "\n",
    "        # flag points that are already moved to the output (condensed)\n",
    "        collapsed[idx] = True\n",
    "  \n",
    "  print(\"%d points remain!\" % len(points) )\n",
    "  value_constraints[ n ] = np.array( points ), np.array( [sv] * len( points ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export constraints to PLY for QAQC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curlew.io import savePLY\n",
    "\n",
    "for k, (p,v) in value_constraints.items():\n",
    "    savePLY('constraints/%s.ply'%k, xyz=p, attr=v[:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load point cloud (of outcrop surface) and use it to compute model extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcrop = loadPLY('./Newcastle_5cm.ply')\n",
    "xyz = outcrop['xyz']\n",
    "extents = np.array( [np.percentile(xyz[:,i], (0,100)) for i in range(3)] ) # extent of outcrop points\n",
    "dims = [ (e[1] - e[0]) for e in extents] # dims to get an 0.5 m grid\n",
    "\n",
    "# create grid for global constraints\n",
    "from curlew.geometry import grid\n",
    "grid_obj = grid(dims, (1,1,1), origin=extents[:,0])\n",
    "cg = grid_obj.coords()\n",
    "grid_obj.sampleArgs = dict(N=512)\n",
    "savePLY('./grid.ply', xyz=cg ) # save for qaqc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a model\n",
    "\n",
    "Now combine the above constraints to build our interpolation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from curlew import HSet\n",
    "from curlew import CSet\n",
    "from curlew.geology import strati, sheet, fault\n",
    "from curlew.geology.model import GeoModel\n",
    "\n",
    "# define shared hyperparamets first\n",
    "H = HSet( value_loss=1.0, grad_loss=1.0, mono_loss=\"0.1\", thick_loss='0.1')\n",
    "params = dict(\n",
    "    input_dim=3, # 3D model\n",
    "    rff_features=64, # number of fourier features\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "\n",
    "# STRATIGRAPHY\n",
    "C0 = CSet( vp=np.vstack( [value_constraints[k][0] for k in ['TopBarBeachShales','TopDudleyCoal', 'TopBogeyHole']] ), # stratigraphic value constraint positions\n",
    "           vv=np.hstack( [value_constraints[k][1] for k in ['TopBarBeachShales','TopDudleyCoal', 'TopBogeyHole']] ), # stratigraphic value constraint values\n",
    "           gp=np.vstack( [gradient_constraints[k][0] for k in ['TopBogeyHole', 'Bedding']] ), # stratigraphic gradient constraint positions\n",
    "           gv=np.vstack( [gradient_constraints[k][1] for k in ['TopBogeyHole', 'Bedding']] ), # stratigraphic gradient constraint vectors\n",
    "           grid=grid_obj, # where to evaluate \"global\" constraints\n",
    "           delta=1 )# grid spacing\n",
    "C0.trend = np.mean( C0.gv, axis=0 ) / np.linalg.norm( np.mean( C0.gv, axis=0 ) ) # specify global trend to encourage flatness\n",
    "\n",
    "s0 = strati('sediments', # interpolator for host rock field\n",
    "            C0, # constraints\n",
    "            H, # hyperparameters\n",
    "            length_scales=[20, 100], # frequencies to use for this structure]\n",
    "            hidden_layers=[8],\n",
    "            **params)\n",
    "# FAULT\n",
    "C1 = CSet( vp=np.vstack( [value_constraints[k][0] for k in ['Fault']] ), # stratigraphic value constraint positions\n",
    "           vv=np.hstack( [value_constraints[k][1] for k in ['Fault']] ), # stratigraphic value constraint values\n",
    "           gp=np.vstack( [gradient_constraints[k][0] for k in ['Fault']] ), # stratigraphic gradient constraint positions\n",
    "           gv=np.vstack( [gradient_constraints[k][1] for k in ['Fault']] ), # stratigraphic gradient constraint vectors\n",
    "           grid=grid_obj, # where to evaluate \"global\" constraints\n",
    "           delta=1) # grid spacing\n",
    "\n",
    "s1 = fault('fault', # interpolator for host rock field\n",
    "            C1, # constraints\n",
    "            H, # hyperparameters\n",
    "            sigma1=(0,0,1),\n",
    "            offset=(1.0, 0.0, 1.0),\n",
    "            length_scales=[10, 100], # frequencies to use for this structure\n",
    "            hidden_layers=[8],\n",
    "            **params)\n",
    "\n",
    "\n",
    "# DYKE\n",
    "C2 = CSet( vp=np.vstack( [value_constraints[k][0] for k in ['Dyke.Upper','Dyke.Lower']] ), # stratigraphic value constraint positions\n",
    "           vv=np.hstack( [value_constraints[k][1] for k in ['Dyke.Upper','Dyke.Lower']] ), # stratigraphic value constraint values\n",
    "           gp=np.vstack( [gradient_constraints[k][0] for k in ['Dyke']] ), # gradient orientation constraints\n",
    "           gv=np.vstack( [gradient_constraints[k][1] for k in ['Dyke']] ), # gradient orientation constraints\n",
    "           grid=grid_obj, # where to evaluate \"global\" constraints\n",
    "           delta=1 ) # grid spacing\n",
    "\n",
    "s2 = sheet('dyke', # interpolator for host rock field\n",
    "            C2, # constraints\n",
    "            H.copy(thick_loss=\"0.01\", mono_loss=\"1.0\"), # value loss is important here!\n",
    "            length_scales=[3,100], # frequencies to use for this structure\n",
    "            contact=(value_constraints['Dyke.Lower'][0], value_constraints['Dyke.Upper'][0]), # scalar values representing the top and bottom of the dyke\n",
    "            hidden_layers=[32],\n",
    "            **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['TopBarBeachShales','TopDudleyCoal', 'TopBogeyHole']:\n",
    "    s0.addIsosurface(name=k, seed=value_constraints[k][0])\n",
    "for k in [\"Dyke.Upper\", \"Dyke.Lower\"]:\n",
    "    s2.addIsosurface(name=k, seed=value_constraints[k][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dyke: 300/300|, value_loss=0.00801, grad_loss=0.00592, thick_loss=0.00971, mono_loss=0.0342\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 223.35it/s]\n",
      "fault: 300/300|, value_loss=0.000162, grad_loss=0.0638, thick_loss=0.000181, mono_loss=0.000288\n",
      "Evaluating: 100%|██████████| 1/1 [00:00<00:00, 181.34it/s]\n",
      "sediments: 300/300|, value_loss=0.167, grad_loss=0.175, thick_loss=0.00433, mono_loss=0.00418, flat_loss=0.0147\n"
     ]
    }
   ],
   "source": [
    "# combine into a geomodel\n",
    "M = GeoModel([s0, s1, s2]) # s1\n",
    "loss = M.prefit( epochs=300 ) # fit scalar fields independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 1000/1000|, dyke=0.0562, fault=0.065, sediments=0.047 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimised fault offset is: 2.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# freeze fault and optimize fault offset\n",
    "if True:\n",
    "    M.freeze([s1, s2]) \n",
    "    M.fit( 1000, learning_rate=1e-1 )\n",
    "    print(\"Optimised fault offset is: %.2f\" % s1.field.offset.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on outcrop surface\n",
    "pred = M.predict(outcrop['xyz'])\n",
    "# savePLY('./outcrop.ply', xyz=outcrop['xyz'], attr=pred ) # save for qaqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model on a grid with 20 cm spacing\n",
    "fine_grid_obj = grid(dims, (0.2,0.2,0.2), origin=extents[:,0])\n",
    "eg = fine_grid_obj.coords()\n",
    "pred = M.predict(eg)\n",
    "# savePLY('./model.ply', xyz=eg, attr=pred ) # save for qaqc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the HD outcrop\n",
    "hd_outcrop = loadPLY(\"Newcastle_1cm.ply\")\n",
    "hd_xyz = hd_outcrop[\"xyz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the classes for the HD Outcrop\n",
    "hd_classes = M.classify(hd_xyz)[0][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise and colormap\n",
    "norm = mcolors.Normalize(vmin=hd_classes.min(), vmax=hd_classes.max())\n",
    "hd_rgba = curlew_cmap(norm(hd_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha compositing\n",
    "def alpha_composite(fg, bg):\n",
    "    \"\"\"\n",
    "    Composites foreground RGBA colors over background RGBA colors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fg : ndarray of shape (N, 4)\n",
    "        Foreground colors (RGBA), with values in [0, 1]\n",
    "    bg : ndarray of shape (N, 4)\n",
    "        Background colors (RGBA), with values in [0, 1]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    out : ndarray of shape (N, 4)\n",
    "        Composited colors (RGBA)\n",
    "    \"\"\"\n",
    "    fg = np.asarray(fg)\n",
    "    bg = np.asarray(bg)\n",
    "\n",
    "    fg_rgb = fg[:, :3]\n",
    "    fg_alpha = fg[:, 3:4]\n",
    "    bg_rgb = bg[:, :3]\n",
    "    bg_alpha = bg[:, 3:4]\n",
    "\n",
    "    out_rgb = fg_rgb * fg_alpha + bg_rgb * (1 - fg_alpha)\n",
    "    out_alpha = fg_alpha + bg_alpha * (1 - fg_alpha)\n",
    "\n",
    "    out = np.concatenate([out_rgb, out_alpha], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite RGB\n",
    "bg = hd_outcrop[\"rgb\"]/255\n",
    "bg = np.c_[bg, np.ones_like(bg[:, 0])]\n",
    "fg = np.c_[hd_rgba[:, :-1], 0.5 * np.ones_like(hd_rgba[:, 0])]\n",
    "comp_rgba = alpha_composite(fg, bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outcrop\n",
    "savePLY(\"./outcrop_classes.ply\", xyz=hd_xyz, rgb=(comp_rgba[:, :-1]*255).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the HD Outcrop\n",
    "predicted_sf_id = M.predict(hd_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the dyke\n",
    "non_dyke = predicted_sf_id[:, 1] > 1\n",
    "non_dyke_xyz = hd_xyz[non_dyke]\n",
    "non_dyke_classes = hd_classes[non_dyke]\n",
    "\n",
    "# Undeform the xyz\n",
    "undeformed_xyz = s1.undeform(non_dyke_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the undeformed point cloud\n",
    "savePLY(\"./outcrop_classes_undeformed.ply\", xyz=undeformed_xyz, rgb=(comp_rgba[non_dyke, :-1]*255).astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
